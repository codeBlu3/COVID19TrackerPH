{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "\n",
    "def get_mb():\n",
    "    try:\n",
    "        mb_ncov_news = []\n",
    "        user_agent = 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:55.0)'\n",
    "\n",
    "\n",
    "        req = Request('https://news.mb.com.ph/tag/ncov', headers={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "                    'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "                    'Accept-Encoding': 'none',\n",
    "                    'Accept-Language': 'en-US,en;q=0.8',\n",
    "                    'Connection': 'keep-alive'})\n",
    "\n",
    "\n",
    "        content = urlopen(req).read()\n",
    "\n",
    "        print('reading url')\n",
    "        soup = BeautifulSoup(content)\n",
    "\n",
    "        print('reading main news section')\n",
    "        mydivs = soup.find(\"main\", {'class':\"tm-content\"})\n",
    "\n",
    "        print('reading urls in news section')\n",
    "        mydivs = mydivs.find_all(\"article\", {'class':\"uk-article listwiththumb\"})\n",
    "        mb_ncov_news += [url.a['href'] for url in mydivs]\n",
    "\n",
    "        last_page = int(soup.find(\"ul\", {'class':\"uk-pagination\"}).text[-3:]) + 1\n",
    "\n",
    "        for i in range(2, last_page):\n",
    "\n",
    "            page_link = '/page' + str(i) + '/'\n",
    "\n",
    "            req = Request('https://news.mb.com.ph/tag/ncov' + page_link, headers={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "                    'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "                    'Accept-Encoding': 'none',\n",
    "                    'Accept-Language': 'en-US,en;q=0.8',\n",
    "                    'Connection': 'keep-alive'})\n",
    "\n",
    "            content = urlopen(req).read()\n",
    "\n",
    "            print('reading url')\n",
    "            soup = BeautifulSoup(content)\n",
    "\n",
    "            print('reading main news section')\n",
    "            mydivs = soup.find(\"main\", {'class':\"tm-content\"})\n",
    "\n",
    "            print('reading urls in news section')\n",
    "            mydivs = mydivs.find_all(\"article\", {'class':\"uk-article listwiththumb\"})\n",
    "            mb_ncov_news += [url.a['href'] for url in mydivs]\n",
    "\n",
    "            print(i)\n",
    "            print(len(mb_ncov_news))\n",
    "            time.sleep(3)\n",
    "\n",
    "        pd.Series(mb_ncov_news).to_csv('m_times_articles.csv', index = False)\n",
    "\n",
    "        mb_df = pd.DataFrame(columns = ['source_id','date','category','title','author','text'])\n",
    "\n",
    "        for article in mb_ncov_news:  \n",
    "            user_agent = 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:55.0) '\n",
    "            req = Request(article, headers={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "                    'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "                    'Accept-Encoding': 'none',\n",
    "                    'Accept-Language': 'en-US,en;q=0.8',\n",
    "                    'Connection': 'keep-alive'})\n",
    "            content = urlopen(req).read()\n",
    "            soup = BeautifulSoup(content)\n",
    "\n",
    "            try:\n",
    "                article_id = ''\n",
    "                date = soup.find('time').text\n",
    "                category = 'nCov'\n",
    "                title = soup.find(\"h1\", {'class':\"uk-article-title uk-margin-bottom-remove\"}).text\n",
    "                author = soup.find('strong').text\n",
    "                art = soup.find_all(\"p\")\n",
    "                text = ' '.join(item.text for item in art)\n",
    "            except AttributeError:\n",
    "                continue\n",
    "\n",
    "            print(title, date)\n",
    "            mb_df = mb_df.append(pd.Series([article_id,date,category, title,author, text], index = mb_df.columns ), ignore_index=True)\n",
    "            time.sleep(3)\n",
    "\n",
    "\n",
    "        mb_df.to_csv('mb_scraped.csv', index = False)\n",
    "        \n",
    "    except:\n",
    "        mb_df.to_csv('new_mb_scraped.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "2\n",
      "12\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "3\n",
      "18\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "4\n",
      "24\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "5\n",
      "30\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "6\n",
      "36\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "7\n",
      "42\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "8\n",
      "48\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "9\n",
      "54\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "10\n",
      "60\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "11\n",
      "66\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "12\n",
      "72\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "13\n",
      "78\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "14\n",
      "84\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "15\n",
      "90\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "16\n",
      "96\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "17\n",
      "102\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "18\n",
      "108\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "19\n",
      "114\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "20\n",
      "120\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "21\n",
      "126\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "22\n",
      "132\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "23\n",
      "138\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "24\n",
      "144\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "25\n",
      "150\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "26\n",
      "156\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "27\n",
      "162\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "28\n",
      "168\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "29\n",
      "174\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "30\n",
      "180\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "31\n",
      "186\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "32\n",
      "192\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "33\n",
      "198\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "34\n",
      "204\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "35\n",
      "210\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "36\n",
      "216\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "37\n",
      "222\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "38\n",
      "228\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "39\n",
      "234\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'mb_df' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-00e5b4c91dfc>\u001b[0m in \u001b[0;36mget_mb\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    640\u001b[0m             response = self.parent.error(\n\u001b[1;32m--> 641\u001b[1;33m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 502: Bad Gateway",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-4c4008a0ebdf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_mb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-00e5b4c91dfc>\u001b[0m in \u001b[0;36mget_mb\u001b[1;34m()\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[0mmb_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'new_mb_scraped.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'mb_df' referenced before assignment"
     ]
    }
   ],
   "source": [
    "get_mb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
