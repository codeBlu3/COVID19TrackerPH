{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "\n",
    "def get_mb():\n",
    "    mb_ncov_news = []\n",
    "    user_agent = 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:55.0)'\n",
    "\n",
    "\n",
    "    req = Request('https://news.mb.com.ph/tag/ncov', headers={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "                'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "                'Accept-Encoding': 'none',\n",
    "                'Accept-Language': 'en-US,en;q=0.8',\n",
    "                'Connection': 'keep-alive'})\n",
    "\n",
    "\n",
    "    content = urlopen(req).read()\n",
    "\n",
    "    print('reading url')\n",
    "    soup = BeautifulSoup(content)\n",
    "\n",
    "    print('reading main news section')\n",
    "    mydivs = soup.find(\"main\", {'class':\"tm-content\"})\n",
    "\n",
    "    print('reading urls in news section')\n",
    "    mydivs = mydivs.find_all(\"article\", {'class':\"uk-article listwiththumb\"})\n",
    "    mb_ncov_news += [url.a['href'] for url in mydivs]\n",
    "\n",
    "    last_page = int(soup.find(\"ul\", {'class':\"uk-pagination\"}).text[-3:]) + 1\n",
    "\n",
    "    for i in range(2, last_page):\n",
    "\n",
    "        page_link = '/page' + str(i) + '/'\n",
    "\n",
    "        req = Request('https://news.mb.com.ph/tag/ncov' + page_link, headers={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "                'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "                'Accept-Encoding': 'none',\n",
    "                'Accept-Language': 'en-US,en;q=0.8',\n",
    "                'Connection': 'keep-alive'})\n",
    "\n",
    "        content = urlopen(req).read()\n",
    "\n",
    "        print('reading url')\n",
    "        soup = BeautifulSoup(content)\n",
    "\n",
    "        print('reading main news section')\n",
    "        mydivs = soup.find(\"main\", {'class':\"tm-content\"})\n",
    "\n",
    "        print('reading urls in news section')\n",
    "        mydivs = mydivs.find_all(\"article\", {'class':\"uk-article listwiththumb\"})\n",
    "        mb_ncov_news += [url.a['href'] for url in mydivs]\n",
    "\n",
    "        print(i)\n",
    "        print(len(mb_ncov_news))\n",
    "        time.sleep(1)\n",
    "        \n",
    "        \n",
    "    mb_df = pd.DataFrame(columns = ['source_id','date','category','title','author','text'])\n",
    "\n",
    "    for article in mb_ncov_news:  \n",
    "        user_agent = 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:55.0) '\n",
    "        req = Request(article, headers={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "                'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "                'Accept-Encoding': 'none',\n",
    "                'Accept-Language': 'en-US,en;q=0.8',\n",
    "                'Connection': 'keep-alive'})\n",
    "        content = urlopen(req).read()\n",
    "        soup = BeautifulSoup(content)\n",
    "\n",
    "        try:\n",
    "            article_id = ''\n",
    "            date = soup.find('time').text\n",
    "            category = 'nCov'\n",
    "            title = soup.find(\"h1\", {'class':\"uk-article-title uk-margin-bottom-remove\"}).text\n",
    "            author = soup.find('strong').text\n",
    "            art = soup.find_all(\"p\")\n",
    "            text = ' '.join(item.text for item in art)\n",
    "        except AttributeError:\n",
    "            continue\n",
    "\n",
    "        print(title, date)\n",
    "        mb_df = mb_df.append(pd.Series([article_id,date,category, title,author, text], index = mb_df.columns ), ignore_index=True)\n",
    "        time.sleep(6)\n",
    "        \n",
    "        \n",
    "    mb_df.to_csv('mb_scraped.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "2\n",
      "12\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "3\n",
      "18\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "4\n",
      "24\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "5\n",
      "30\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "6\n",
      "36\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "7\n",
      "42\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "8\n",
      "48\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "9\n",
      "54\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "10\n",
      "60\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "11\n",
      "66\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "12\n",
      "72\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "13\n",
      "78\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "14\n",
      "84\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "15\n",
      "90\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "16\n",
      "96\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "17\n",
      "102\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "18\n",
      "108\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "19\n",
      "114\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "20\n",
      "120\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "21\n",
      "126\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "22\n",
      "132\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "23\n",
      "138\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "24\n",
      "144\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "25\n",
      "150\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "26\n",
      "156\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "27\n",
      "162\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "28\n",
      "168\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "29\n",
      "174\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "30\n",
      "180\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "31\n",
      "186\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "32\n",
      "192\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "33\n",
      "198\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "34\n",
      "204\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "35\n",
      "210\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "36\n",
      "216\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "37\n",
      "222\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "38\n",
      "228\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "39\n",
      "234\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "40\n",
      "240\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "41\n",
      "246\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "42\n",
      "252\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "43\n",
      "258\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "44\n",
      "264\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "45\n",
      "270\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "46\n",
      "276\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "47\n",
      "282\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "48\n",
      "288\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "49\n",
      "294\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "50\n",
      "300\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "51\n",
      "306\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "52\n",
      "312\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "53\n",
      "318\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "54\n",
      "324\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "55\n",
      "330\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "56\n",
      "336\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "57\n",
      "342\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "58\n",
      "348\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "59\n",
      "354\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "60\n",
      "360\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "61\n",
      "366\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "62\n",
      "372\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "63\n",
      "378\n",
      "reading url\n",
      "reading main news section\n",
      "reading urls in news section\n",
      "64\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "get_mb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
