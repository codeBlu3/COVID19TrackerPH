{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jrdaos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from dateutil import parser\n",
    "import re\n",
    "from geotext import GeoText\n",
    "import numpy as np\n",
    "from scipy import stats \n",
    "import geopandas as gpd\n",
    "\n",
    "# Spacy for tokenizing our texts\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Gensim is needed for modeling\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_punctuation, strip_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Spacy Tokenizer\n",
    "nlp = English()\n",
    "\n",
    "def lemmatizer(doc):\n",
    "    # This takes in a doc of tokens from the NER and lemmatizes them. \n",
    "    # Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "    doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)\n",
    "    \n",
    "def remove_stopwords(doc):\n",
    "    # This will remove stopwords and punctuation.\n",
    "    # Use token.text to return strings, which we'll need for Gensim.\n",
    "    doc = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "    return doc\n",
    "\n",
    "# This will add pipelines in our tokenization process.\n",
    "\n",
    "nlp.add_pipe(lemmatizer,name='lemmatizer')\n",
    "nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This is a function that will create a model that predicts the topics conveyed by each group of tweet sentiments\n",
    "\n",
    "\n",
    "def topic_modeler(tokenized_texts, no_topics, no_words):\n",
    "    topics = []\n",
    "\n",
    "    words = corpora.Dictionary(tokenized_texts)\n",
    "    corpus = [words.doc2bow(doc) for doc in tokenized_texts]\n",
    "\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                               id2word=words,\n",
    "                                                random_state = 3,\n",
    "                                               num_topics= no_topics)\n",
    "    \n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=tokenized_texts, dictionary=words, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('rappler_scraping.csv')\n",
    "df = df.iloc[:,1:]\n",
    "df['date'] = [parser.parse(date).strftime('%Y-%m-%d') for date in df['date']]\n",
    "df = df[(df['text'].str.contains('coronavirus'))]\n",
    "df = df[df['category'] == 'Philippines']\n",
    "df = df.drop_duplicates()\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "\n",
    "\n",
    "location = pd.read_csv('ph_locations.csv')\n",
    "location = location.applymap(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA Topics \n",
    "\n",
    "words = df['text'].str.lower()\n",
    "listWords = []\n",
    "for item in words:\n",
    "    listWords.append([nlp(item)])\n",
    "\n",
    "topics = []\n",
    "for x in listWords:\n",
    "    res = topic_modeler(x, 1, 30)\n",
    "    res = res.show_topic(0, topn = 30)\n",
    "    topics.append([word[0] for word in res])\n",
    "    \n",
    "df['LDA_Topics'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting all the counting phrases in the articles\n",
    "\n",
    "df['count_docs'] =  df['text'].apply(lambda x: re.findall(\"\\d+(?:,\\d+)?\\s+[a-zA-Z]+\", x))\n",
    "\n",
    "checker = ['confirmed','suspected','quarantine','case','infected','monitoring','chinese','monitored']\n",
    "\n",
    "count_docs = []\n",
    "for index, row in df.iterrows():\n",
    "    passed = []\n",
    "    for item in row['count_docs']:\n",
    "        if any(ext in item.lower() for ext in checker):\n",
    "            passed.append(item)\n",
    "            break\n",
    "    \n",
    "    count_docs.append(passed)\n",
    "\n",
    "df['count_docs'] = count_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting all the PH Locations using geotext on the articles\n",
    "\n",
    "df['PH_Loc'] = [list(set(GeoText(content, 'PH').cities)) for content in df['text']]\n",
    "df['PH_Loc'] = [[x.lower() for x in w] for w in df['PH_Loc']]\n",
    "df['PH_Loc'] =[[x.replace('city', '') for x in w] for w in df['PH_Loc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying which articles are about suspicious or confirmed cases of the virus\n",
    "\n",
    "status = []\n",
    "for index, row in df.iterrows():\n",
    "    if ('confirmed' in row['LDA_Topics']) & ('confirm' in row['title'])  & (row['date'] >= '2020-01-30'):\n",
    "        status.append('confirmed')\n",
    "    elif ('confirmed' in row['LDA_Topics']) & (row['date'] >= '2020-01-30'):\n",
    "        status.append('confirmed')\n",
    "    elif (any(words in row['LDA_Topics']  for words in ['suspected','quarantine','case','infected','monitoring']))& ('FACT CHECK' not in row['title']) & ('FALSE' not in row['title']):\n",
    "        status.append('suspected')\n",
    "    else:\n",
    "        status.append('')\n",
    "df['status'] = status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Provinces in the identified locations\n",
    "\n",
    "df['PH_Loc'] = [list(set(loc) & set(location['Pro_Name'].unique())) for loc in df['PH_Loc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For locations not identified through the text, it will check with the LDA topics if a location is identified and use it instead\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if len(row['PH_Loc']) == 0:\n",
    "        try:\n",
    "            df.loc[index, 'PH_Loc'] = [list(set(row['LDA_Topics']) & set(location['Pro_Name'].unique()))]\n",
    "        except ValueError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the document counts to just numbers\n",
    "\n",
    "counts = []\n",
    "case = []\n",
    "for count in df['count_docs']:\n",
    "    try:\n",
    "        counts.append(count[0].split(' ')[0])\n",
    "    except IndexError:\n",
    "        counts.append(0)\n",
    "        \n",
    "    try:\n",
    "        case.append(count[0].split(' ')[1])\n",
    "    except IndexError:\n",
    "        case.append('')\n",
    "\n",
    "df['counts'] = counts\n",
    "df['counts'] = [str(count).replace(',', '') for count in df['counts']]\n",
    "df['counts'] = [str(count).replace('.', '') for count in df['counts']]\n",
    "df['case'] = case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalizing Locations\n",
    "\n",
    "ph_loc = []\n",
    "for loc in df['PH_Loc']:\n",
    "    try:\n",
    "        ph_loc.append(loc[0])\n",
    "    except IndexError:\n",
    "        ph_loc.append('')\n",
    "df['Loc'] = ph_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing confirmed counts\n",
    "\n",
    "count_fixer = []\n",
    "for index, row in df.iterrows():\n",
    "    if (row['status'] == 'confirmed') & (row['case'] != 'confirmed'):\n",
    "        count_fixer.append(1)\n",
    "    else:\n",
    "        count_fixer.append(row['counts'])\n",
    "\n",
    "df['counts'] = count_fixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_id</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>LDA_Topics</th>\n",
       "      <th>count_docs</th>\n",
       "      <th>PH_Loc</th>\n",
       "      <th>status</th>\n",
       "      <th>counts</th>\n",
       "      <th>case</th>\n",
       "      <th>Loc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>249832</td>\n",
       "      <td>2020-01-21</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>DOH probes suspected case of novel coronavirus...</td>\n",
       "      <td>Janella Paris</td>\n",
       "      <td>(3rd UPDATE) Health Secretary Francisco Duque ...</td>\n",
       "      <td>[health, said, sars, duque, coronavirus, chine...</td>\n",
       "      <td>[3 Chinese]</td>\n",
       "      <td>[]</td>\n",
       "      <td>suspected</td>\n",
       "      <td>3</td>\n",
       "      <td>Chinese</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>249914</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>Duque: Still many blindspots in ‘novel coronav...</td>\n",
       "      <td>Janella Paris</td>\n",
       "      <td>Duque says being infected with a coronavirus i...</td>\n",
       "      <td>[said, duque, coronavirus, coronaviruses, chil...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>suspected</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>249978</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>Cebu Pacific calls on HK flight passengers to ...</td>\n",
       "      <td>Rappler.com</td>\n",
       "      <td>This is one of the airline's precautionary mea...</td>\n",
       "      <td>[hong, kong, cebu, pacific, flight, january, m...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[cebu]</td>\n",
       "      <td>suspected</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>cebu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>249996</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>Stay vigilant vs novel coronavirus, PHO-Aklan ...</td>\n",
       "      <td>Boy Ryan B. Zabal</td>\n",
       "      <td>Kalibo gets international flights daily, mostl...</td>\n",
       "      <td>[aklan, kalibo, china, chinese, hospital, coro...</td>\n",
       "      <td>[3 Chinese]</td>\n",
       "      <td>[aklan]</td>\n",
       "      <td>suspected</td>\n",
       "      <td>3</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>aklan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>250017</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>PH tracks down, now monitoring family of HK's ...</td>\n",
       "      <td>Lian Buan</td>\n",
       "      <td>An Advanced Passenger Information System has b...</td>\n",
       "      <td>[said, coronavirus, family, bi, wuhan, duque, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[cebu]</td>\n",
       "      <td>suspected</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>cebu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>250783</td>\n",
       "      <td>2020-02-02</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>Duterte expands ban on travelers from China, H...</td>\n",
       "      <td>Sofia Tomacruz</td>\n",
       "      <td>(UPDATED) Only Filipino citizens and permanent...</td>\n",
       "      <td>[china, ban, medialdea, philippines, coming, s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>suspected</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>250795</td>\n",
       "      <td>2020-02-02</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>Robredo urges Filipinos to respect Chinese ami...</td>\n",
       "      <td>Sofia Tomacruz</td>\n",
       "      <td>'Walang iisang may kasalanan tayo, kaya tayo, ...</td>\n",
       "      <td>[iyong, robredo, na, philippines, coronavirus,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>suspected</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>250817</td>\n",
       "      <td>2020-02-02</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>PH expands visa suspension for travelers from ...</td>\n",
       "      <td>Rappler.com</td>\n",
       "      <td>The Philippines temporarily stops issuing visa...</td>\n",
       "      <td>[philippines, china, coming, hong, kong, suspe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>186633</td>\n",
       "      <td>2020-01-30</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>Philippines confirms 1st case of novel coronav...</td>\n",
       "      <td>Janelle Paris</td>\n",
       "      <td>(3rd UPDATE) The Philippines' first 2019-nCoV ...</td>\n",
       "      <td>[health, philippines, january, 2019-ncov, doh,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>confirmed</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>251003</td>\n",
       "      <td>2020-02-05</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>Philippines confirms 3rd case of novel coronav...</td>\n",
       "      <td>Janella Paris</td>\n",
       "      <td>(6th UPDATE) The Department of Health is probi...</td>\n",
       "      <td>[january, said, china, doh, february, 2019-nco...</td>\n",
       "      <td>[133 cases]</td>\n",
       "      <td>[cebu]</td>\n",
       "      <td>confirmed</td>\n",
       "      <td>1</td>\n",
       "      <td>cases</td>\n",
       "      <td>cebu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    source_id        date     category  \\\n",
       "0      249832  2020-01-21  Philippines   \n",
       "1      249914  2020-01-22  Philippines   \n",
       "2      249978  2020-01-23  Philippines   \n",
       "3      249996  2020-01-23  Philippines   \n",
       "4      250017  2020-01-23  Philippines   \n",
       "..        ...         ...          ...   \n",
       "63     250783  2020-02-02  Philippines   \n",
       "64     250795  2020-02-02  Philippines   \n",
       "65     250817  2020-02-02  Philippines   \n",
       "66     186633  2020-01-30  Philippines   \n",
       "67     251003  2020-02-05  Philippines   \n",
       "\n",
       "                                                title             author  \\\n",
       "0   DOH probes suspected case of novel coronavirus...      Janella Paris   \n",
       "1   Duque: Still many blindspots in ‘novel coronav...      Janella Paris   \n",
       "2   Cebu Pacific calls on HK flight passengers to ...        Rappler.com   \n",
       "3   Stay vigilant vs novel coronavirus, PHO-Aklan ...  Boy Ryan B. Zabal   \n",
       "4   PH tracks down, now monitoring family of HK's ...          Lian Buan   \n",
       "..                                                ...                ...   \n",
       "63  Duterte expands ban on travelers from China, H...     Sofia Tomacruz   \n",
       "64  Robredo urges Filipinos to respect Chinese ami...     Sofia Tomacruz   \n",
       "65  PH expands visa suspension for travelers from ...        Rappler.com   \n",
       "66  Philippines confirms 1st case of novel coronav...      Janelle Paris   \n",
       "67  Philippines confirms 3rd case of novel coronav...      Janella Paris   \n",
       "\n",
       "                                                 text  \\\n",
       "0   (3rd UPDATE) Health Secretary Francisco Duque ...   \n",
       "1   Duque says being infected with a coronavirus i...   \n",
       "2   This is one of the airline's precautionary mea...   \n",
       "3   Kalibo gets international flights daily, mostl...   \n",
       "4   An Advanced Passenger Information System has b...   \n",
       "..                                                ...   \n",
       "63  (UPDATED) Only Filipino citizens and permanent...   \n",
       "64  'Walang iisang may kasalanan tayo, kaya tayo, ...   \n",
       "65  The Philippines temporarily stops issuing visa...   \n",
       "66  (3rd UPDATE) The Philippines' first 2019-nCoV ...   \n",
       "67  (6th UPDATE) The Department of Health is probi...   \n",
       "\n",
       "                                           LDA_Topics   count_docs   PH_Loc  \\\n",
       "0   [health, said, sars, duque, coronavirus, chine...  [3 Chinese]       []   \n",
       "1   [said, duque, coronavirus, coronaviruses, chil...           []       []   \n",
       "2   [hong, kong, cebu, pacific, flight, january, m...           []   [cebu]   \n",
       "3   [aklan, kalibo, china, chinese, hospital, coro...  [3 Chinese]  [aklan]   \n",
       "4   [said, coronavirus, family, bi, wuhan, duque, ...           []   [cebu]   \n",
       "..                                                ...          ...      ...   \n",
       "63  [china, ban, medialdea, philippines, coming, s...           []       []   \n",
       "64  [iyong, robredo, na, philippines, coronavirus,...           []       []   \n",
       "65  [philippines, china, coming, hong, kong, suspe...           []       []   \n",
       "66  [health, philippines, january, 2019-ncov, doh,...           []       []   \n",
       "67  [january, said, china, doh, february, 2019-nco...  [133 cases]   [cebu]   \n",
       "\n",
       "       status counts     case    Loc  \n",
       "0   suspected      3  Chinese         \n",
       "1   suspected      0                  \n",
       "2   suspected      0            cebu  \n",
       "3   suspected      3  Chinese  aklan  \n",
       "4   suspected      0            cebu  \n",
       "..        ...    ...      ...    ...  \n",
       "63  suspected      0                  \n",
       "64  suspected      0                  \n",
       "65                 0                  \n",
       "66  confirmed      1                  \n",
       "67  confirmed      1    cases   cebu  \n",
       "\n",
       "[68 rows x 13 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing for CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop = True)\n",
    "df.to_csv('rappler_parsed.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('rappler_parsed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(df):\n",
    "    print(df.info())\n",
    "    \n",
    "    # Get min/max/mean values\n",
    "    dfa = pd.pivot_table(df, values = 'counts', index=['date', 'Loc'], columns='status', aggfunc=[min, max, np.mean, stats.mode])\n",
    "    \n",
    "    # Remove multi-index\n",
    "    dfa.columns = [\"_\".join(pair) for pair in dfa.columns]\n",
    "    dfa = dfa.reset_index()\n",
    "    \n",
    "    # Replace 0 with np.nan to forward fill null values\n",
    "    dfa = dfa.replace(0, np.nan)\n",
    "    \n",
    "    # Forward filling needs to be by area\n",
    "    places = list(df['Loc'].unique())\n",
    "    \n",
    "    global dfb\n",
    "    dfb = pd.DataFrame()\n",
    "    for place in places:\n",
    "        df_temp = dfa[dfa['Loc'] == place].fillna(method='ffill')\n",
    "        dfb = dfb.append(df_temp)\n",
    "    return dfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 68 entries, 0 to 67\n",
      "Data columns (total 13 columns):\n",
      "source_id     68 non-null int64\n",
      "date          68 non-null object\n",
      "category      68 non-null object\n",
      "title         68 non-null object\n",
      "author        68 non-null object\n",
      "text          68 non-null object\n",
      "LDA_Topics    68 non-null object\n",
      "count_docs    68 non-null object\n",
      "PH_Loc        68 non-null object\n",
      "status        31 non-null object\n",
      "counts        68 non-null int64\n",
      "case          17 non-null object\n",
      "Loc           30 non-null object\n",
      "dtypes: int64(2), object(11)\n",
      "memory usage: 7.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "res = parse(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res[['date','Loc', 'min_suspected','min_confirmed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "prov = gpd.read_file('provinces.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "prov['Pro_Name'] = prov['Pro_Name'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(res, prov, left_on = 'Loc', right_on = 'Pro_Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid field type <class 'pandas._libs.tslibs.timestamps.Timestamp'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-121-b94c1a216a6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"output.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"GeoJSON\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\geopandas\\geodataframe.py\u001b[0m in \u001b[0;36mto_file\u001b[1;34m(self, filename, driver, schema, **kwargs)\u001b[0m\n\u001b[0;32m    411\u001b[0m         \"\"\"\n\u001b[0;32m    412\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mgeopandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m         \u001b[0mto_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_crs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\geopandas\\io\\file.py\u001b[0m in \u001b[0;36mto_file\u001b[1;34m(df, filename, driver, schema, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m         with fiona.open(filename, 'w', driver=driver, crs=df.crs,\n\u001b[0;32m    116\u001b[0m                         schema=schema, **kwargs) as colxn:\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0mcolxn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterecords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\fiona\\collection.py\u001b[0m in \u001b[0;36mwriterecords\u001b[1;34m(self, records)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"collection not open for writing\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterecs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bounds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_extent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mfiona/ogrext.pyx\u001b[0m in \u001b[0;36mfiona.ogrext.WritingSession.writerecs\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mfiona/ogrext.pyx\u001b[0m in \u001b[0;36mfiona.ogrext.OGRFeatureBuilder.build\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid field type <class 'pandas._libs.tslibs.timestamps.Timestamp'>"
     ]
    }
   ],
   "source": [
    "df2.to_file(\"output.json\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Loc</th>\n",
       "      <th>min_suspected</th>\n",
       "      <th>min_confirmed</th>\n",
       "      <th>Pro_Name</th>\n",
       "      <th>long</th>\n",
       "      <th>lat</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>cebu</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cebu</td>\n",
       "      <td>123.754729</td>\n",
       "      <td>10.355054</td>\n",
       "      <td>(POLYGON ((123.3867917460001 9.434405029000061...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>cebu</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cebu</td>\n",
       "      <td>123.754729</td>\n",
       "      <td>10.355054</td>\n",
       "      <td>(POLYGON ((123.3867917460001 9.434405029000061...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-30</td>\n",
       "      <td>cebu</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cebu</td>\n",
       "      <td>123.754729</td>\n",
       "      <td>10.355054</td>\n",
       "      <td>(POLYGON ((123.3867917460001 9.434405029000061...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>cebu</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cebu</td>\n",
       "      <td>123.754729</td>\n",
       "      <td>10.355054</td>\n",
       "      <td>(POLYGON ((123.3867917460001 9.434405029000061...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-02-02</td>\n",
       "      <td>cebu</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cebu</td>\n",
       "      <td>123.754729</td>\n",
       "      <td>10.355054</td>\n",
       "      <td>(POLYGON ((123.3867917460001 9.434405029000061...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-02-05</td>\n",
       "      <td>cebu</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cebu</td>\n",
       "      <td>123.754729</td>\n",
       "      <td>10.355054</td>\n",
       "      <td>(POLYGON ((123.3867917460001 9.434405029000061...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>aklan</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>aklan</td>\n",
       "      <td>122.248020</td>\n",
       "      <td>11.609596</td>\n",
       "      <td>(POLYGON ((122.4047963630001 11.64533761000007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-01-25</td>\n",
       "      <td>aklan</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>aklan</td>\n",
       "      <td>122.248020</td>\n",
       "      <td>11.609596</td>\n",
       "      <td>(POLYGON ((122.4047963630001 11.64533761000007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-01-28</td>\n",
       "      <td>aklan</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>aklan</td>\n",
       "      <td>122.248020</td>\n",
       "      <td>11.609596</td>\n",
       "      <td>(POLYGON ((122.4047963630001 11.64533761000007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-01-30</td>\n",
       "      <td>aklan</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>aklan</td>\n",
       "      <td>122.248020</td>\n",
       "      <td>11.609596</td>\n",
       "      <td>(POLYGON ((122.4047963630001 11.64533761000007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>aklan</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>aklan</td>\n",
       "      <td>122.248020</td>\n",
       "      <td>11.609596</td>\n",
       "      <td>(POLYGON ((122.4047963630001 11.64533761000007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>camiguin</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>camiguin</td>\n",
       "      <td>124.717743</td>\n",
       "      <td>9.171998</td>\n",
       "      <td>(POLYGON ((124.825425739 9.172776295000062, 12...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date       Loc  min_suspected  min_confirmed  Pro_Name        long  \\\n",
       "0  2020-01-23      cebu            0.0            0.0      cebu  123.754729   \n",
       "1  2020-01-24      cebu           14.0            0.0      cebu  123.754729   \n",
       "2  2020-01-30      cebu           14.0            0.0      cebu  123.754729   \n",
       "3  2020-01-31      cebu          100.0            1.0      cebu  123.754729   \n",
       "4  2020-02-02      cebu          100.0            1.0      cebu  123.754729   \n",
       "5  2020-02-05      cebu          100.0            1.0      cebu  123.754729   \n",
       "6  2020-01-23     aklan            3.0            0.0     aklan  122.248020   \n",
       "7  2020-01-25     aklan           80.0            0.0     aklan  122.248020   \n",
       "8  2020-01-28     aklan           11.0            0.0     aklan  122.248020   \n",
       "9  2020-01-30     aklan           11.0            1.0     aklan  122.248020   \n",
       "10 2020-01-31     aklan           11.0            1.0     aklan  122.248020   \n",
       "11 2020-01-27  camiguin          106.0            0.0  camiguin  124.717743   \n",
       "\n",
       "          lat                                           geometry  \n",
       "0   10.355054  (POLYGON ((123.3867917460001 9.434405029000061...  \n",
       "1   10.355054  (POLYGON ((123.3867917460001 9.434405029000061...  \n",
       "2   10.355054  (POLYGON ((123.3867917460001 9.434405029000061...  \n",
       "3   10.355054  (POLYGON ((123.3867917460001 9.434405029000061...  \n",
       "4   10.355054  (POLYGON ((123.3867917460001 9.434405029000061...  \n",
       "5   10.355054  (POLYGON ((123.3867917460001 9.434405029000061...  \n",
       "6   11.609596  (POLYGON ((122.4047963630001 11.64533761000007...  \n",
       "7   11.609596  (POLYGON ((122.4047963630001 11.64533761000007...  \n",
       "8   11.609596  (POLYGON ((122.4047963630001 11.64533761000007...  \n",
       "9   11.609596  (POLYGON ((122.4047963630001 11.64533761000007...  \n",
       "10  11.609596  (POLYGON ((122.4047963630001 11.64533761000007...  \n",
       "11   9.171998  (POLYGON ((124.825425739 9.172776295000062, 12...  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = gpd.GeoDataFrame(\n",
    "    df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['date','Loc','min_suspected','min_confirmed','long','lat']]\n",
    "df.columns = (['Date','Location','Suspected','Confirmed','Longitude','Latitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ncov_parsed.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>Suspected</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-23T00:00:00.000000</td>\n",
       "      <td>cebu</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123.754729</td>\n",
       "      <td>10.355054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-01-23T00:00:00.000000</td>\n",
       "      <td>aklan</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>122.248020</td>\n",
       "      <td>11.609596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-24T00:00:00.000000</td>\n",
       "      <td>cebu</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123.754729</td>\n",
       "      <td>10.355054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-01-25T00:00:00.000000</td>\n",
       "      <td>aklan</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>122.248020</td>\n",
       "      <td>11.609596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2020-01-27T00:00:00.000000</td>\n",
       "      <td>camiguin</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>124.717743</td>\n",
       "      <td>9.171998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-01-28T00:00:00.000000</td>\n",
       "      <td>aklan</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>122.248020</td>\n",
       "      <td>11.609596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-30T00:00:00.000000</td>\n",
       "      <td>cebu</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123.754729</td>\n",
       "      <td>10.355054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-01-30T00:00:00.000000</td>\n",
       "      <td>aklan</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>122.248020</td>\n",
       "      <td>11.609596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-31T00:00:00.000000</td>\n",
       "      <td>cebu</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>123.754729</td>\n",
       "      <td>10.355054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020-01-31T00:00:00.000000</td>\n",
       "      <td>aklan</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>122.248020</td>\n",
       "      <td>11.609596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-02-02T00:00:00.000000</td>\n",
       "      <td>cebu</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>123.754729</td>\n",
       "      <td>10.355054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-02-05T00:00:00.000000</td>\n",
       "      <td>cebu</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>123.754729</td>\n",
       "      <td>10.355054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Date  Location  Suspected  Confirmed   Longitude  \\\n",
       "0   2020-01-23T00:00:00.000000      cebu        0.0        0.0  123.754729   \n",
       "6   2020-01-23T00:00:00.000000     aklan        3.0        0.0  122.248020   \n",
       "1   2020-01-24T00:00:00.000000      cebu       14.0        0.0  123.754729   \n",
       "7   2020-01-25T00:00:00.000000     aklan       80.0        0.0  122.248020   \n",
       "11  2020-01-27T00:00:00.000000  camiguin      106.0        0.0  124.717743   \n",
       "8   2020-01-28T00:00:00.000000     aklan       11.0        0.0  122.248020   \n",
       "2   2020-01-30T00:00:00.000000      cebu       14.0        0.0  123.754729   \n",
       "9   2020-01-30T00:00:00.000000     aklan       11.0        1.0  122.248020   \n",
       "3   2020-01-31T00:00:00.000000      cebu      100.0        1.0  123.754729   \n",
       "10  2020-01-31T00:00:00.000000     aklan       11.0        1.0  122.248020   \n",
       "4   2020-02-02T00:00:00.000000      cebu      100.0        1.0  123.754729   \n",
       "5   2020-02-05T00:00:00.000000      cebu      100.0        1.0  123.754729   \n",
       "\n",
       "     Latitude  \n",
       "0   10.355054  \n",
       "6   11.609596  \n",
       "1   10.355054  \n",
       "7   11.609596  \n",
       "11   9.171998  \n",
       "8   11.609596  \n",
       "2   10.355054  \n",
       "9   11.609596  \n",
       "3   10.355054  \n",
       "10  11.609596  \n",
       "4   10.355054  \n",
       "5   10.355054  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = [datetime.datetime.strptime(str(date), '%Y-%m-%d').strftime('%Y-%m-%dT%H:%M:%S.%f') for date in df['date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date             datetime64[ns]\n",
       "Loc                      object\n",
       "min_suspected           float64\n",
       "min_confirmed           float64\n",
       "Pro_Name                 object\n",
       "long                    float64\n",
       "lat                     float64\n",
       "geometry                 object\n",
       "dtype: object"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
